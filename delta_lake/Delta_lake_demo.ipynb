{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54bee82b-9701-426a-92f9-c68fc9759d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake: ACID Properties & Time Travel Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Creating Delta tables with ACID guarantees\n",
    "- Concurrent writes with isolation\n",
    "- Time Travel for auditing and rollback\n",
    "- Schema enforcement\n",
    "- Performance optimization tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34639981-5458-455a-9067-a4789b668765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup: Initialize Spark with Delta Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "075234e8-2be7-4f7e-8ed3-83b80a8b5a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "import time\n",
    "\n",
    "# Enable Delta optimizations\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "# Set up storage path (adjust for your environment)\n",
    "delta_path = \"/Volumes/workspace/siddatasets/filedatasets/landing/deltademo/\"\n",
    "\n",
    "# Clean up if exists from previous runs\n",
    "dbutils.fs.rm(delta_path, recurse=True)\n",
    "\n",
    "print(\"‚úÖ Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5b04e4-8451-403f-98af-037fc741004b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Create Your First Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6b356b-9faa-4a2d-8aab-57e4d4ee739b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample customer data\n",
    "data = [\n",
    "    (1, \"Acme Corp\", 50000, \"2026-01-15\"),\n",
    "    (2, \"TechStart Inc\", 75000, \"2026-01-20\"),\n",
    "    (3, \"Global Industries\", 120000, \"2026-01-25\"),\n",
    "    (4, \"DataFlow Systems\", 95000, \"2026-01-28\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"company_name\", \"revenue\", \"transaction_date\"])\n",
    "\n",
    "# Write as Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "print(\"‚úÖ Delta table created\")\n",
    "display(spark.read.format(\"delta\").load(delta_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3bb5a30-58bd-4a02-9d1e-c79a64896355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: ACID - Isolation (Concurrent Writes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3ec3a7-5a7d-4d64-b917-0649556cfddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Simulate two jobs updating the same customer simultaneously\n",
    "print(\"üìù Job 1: Updating revenue for customer_id = 1\")\n",
    "delta_table.update(\n",
    "    condition = \"customer_id = 1\",\n",
    "    set = {\"revenue\": \"55000\"}\n",
    ")\n",
    "\n",
    "print(\"üìù Job 2: Updating company name for customer_id = 1\")\n",
    "delta_table.update(\n",
    "    condition = \"customer_id = 1\",\n",
    "    set = {\"company_name\": \"Acme Corporation\"}\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Both updates succeeded without conflicts!\")\n",
    "print(\"Current state of customer_id = 1:\")\n",
    "display(spark.read.format(\"delta\").load(delta_path).filter(\"customer_id = 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9442e41-a5c0-4722-bcd8-c0412bd44cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: ACID - Atomicity (All or Nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9b97a6-4bc5-4a05-b79e-f35b3b368fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Attempt a batch update - if ANY row fails, ALL rows rollback\n",
    "try:\n",
    "    delta_table.update(\n",
    "        condition = \"revenue > 50000\",\n",
    "        set = {\"revenue\": \"revenue * 1.1\"}  # 10% increase\n",
    "    )\n",
    "    print(\"‚úÖ Batch update completed atomically - all qualifying rows updated\")\n",
    "    display(spark.read.format(\"delta\").load(delta_path))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Update failed and rolled back: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "058194fc-33e2-473d-998e-c3ebbd0ad306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: Time Travel - Version History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32da44b5-a1c7-4420-b7a9-6e39569e2bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View complete transaction history\n",
    "print(\"üìú Transaction History:\")\n",
    "display(delta_table.history())\n",
    "\n",
    "# Show key columns\n",
    "print(\"\\nüìä Summary of Changes:\")\n",
    "display(delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2753ce3a-5aca-458c-a6a6-a0dc0c71d009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Time Travel - Read Previous Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc90ceec-0b05-4d59-8f68-500aca60d467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read version 0 (original data)\n",
    "print(\"üìÇ Version 0 (Original):\")\n",
    "v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "display(v0.filter(\"customer_id = 1\"))\n",
    "\n",
    "# Read version 1 (after first update)\n",
    "print(\"\\nüìÇ Version 1 (After revenue update):\")\n",
    "v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_path)\n",
    "display(v1.filter(\"customer_id = 1\"))\n",
    "\n",
    "# Read current version\n",
    "print(\"\\nüìÇ Current Version:\")\n",
    "current = spark.read.format(\"delta\").load(delta_path)\n",
    "display(current.filter(\"customer_id = 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc249775-dfcd-457b-9c81-f6a6ab862945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 6: Time Travel by Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23ca9301-c0fd-458c-8c25-f53b8d7d5f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get timestamp from 5 minutes ago (adjust as needed)\n",
    "five_min_ago = (datetime.now() - timedelta(minutes=5)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(f\"üìÖ Reading data as of: {five_min_ago}\")\n",
    "\n",
    "try:\n",
    "    historical_df = spark.read.format(\"delta\") \\\n",
    "        .option(\"timestampAsOf\", five_min_ago) \\\n",
    "        .load(delta_path)\n",
    "    display(historical_df)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Timestamp too old or before table creation: {e}\")\n",
    "    print(\"Showing version 0 instead:\")\n",
    "    display(spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9223265-b009-48b6-b4a6-9f47c58c5cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 7: Schema Enforcement (Consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dc5de0f-04de-44ad-883a-1dbfa9dcbe76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to insert data with WRONG schema\n",
    "print(\"üß™ Attempting to insert bad data (string in revenue column)...\")\n",
    "\n",
    "bad_data = [(5, \"Invalid Corp\", \"NOT_A_NUMBER\", \"2026-02-01\")]\n",
    "bad_df = spark.createDataFrame(bad_data, [\"customer_id\", \"company_name\", \"revenue\", \"transaction_date\"])\n",
    "\n",
    "try:\n",
    "    bad_df.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "    print(\"‚ùå This shouldn't print - bad data was accepted!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Delta correctly rejected bad data!\")\n",
    "    print(f\"Error: {str(e)[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93e269cf-62aa-4acd-9e28-92148fb99224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to insert data with MISSING column\n",
    "print(\"\\nüß™ Attempting to insert data with missing column...\")\n",
    "\n",
    "incomplete_data = [(6, \"Incomplete Corp\", 80000)]\n",
    "incomplete_df = spark.createDataFrame(incomplete_data, [\"customer_id\", \"company_name\", \"revenue\"])\n",
    "\n",
    "try:\n",
    "    incomplete_df.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "    print(\"‚ùå This shouldn't print - incomplete data was accepted!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Delta correctly rejected incomplete data!\")\n",
    "    print(f\"Error: {str(e)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9eb518-b18f-47d3-b8f2-fa0654b78647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 8: Schema Evolution (Controlled Changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47eb13dd-e316-4f44-84ed-4a9622944ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column using schema evolution\n",
    "new_data = [\n",
    "    (5, \"Quantum Systems\", 110000, \"2026-02-01\", \"Enterprise\")\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(\n",
    "    new_data, \n",
    "    [\"customer_id\", \"company_name\", \"revenue\", \"transaction_date\", \"customer_tier\"]\n",
    ")\n",
    "\n",
    "print(\"üìù Adding data with NEW column (customer_tier)...\")\n",
    "\n",
    "# Enable schema evolution\n",
    "new_df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(\"‚úÖ Schema evolved successfully!\")\n",
    "display(spark.read.format(\"delta\").load(delta_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75eeb275-4ab8-417e-80a7-a2ce6ee6c23b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 9: Merge (Upsert) Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cc852a8-f748-40be-b01e-1c55747cf6ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare updates and new records\n",
    "updates_data = [\n",
    "    (1, \"Acme Corporation Ltd\", 60000, \"2026-02-01\", \"Premium\"),  # Update existing\n",
    "    (6, \"NewCo Industries\", 85000, \"2026-02-01\", \"Standard\")      # Insert new\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(\n",
    "    updates_data,\n",
    "    [\"customer_id\", \"company_name\", \"revenue\", \"transaction_date\", \"customer_tier\"]\n",
    ")\n",
    "\n",
    "print(\"üîÑ Performing MERGE (upsert) operation...\")\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    updates_df.alias(\"source\"),\n",
    "    \"target.customer_id = source.customer_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"company_name\": \"source.company_name\",\n",
    "        \"revenue\": \"source.revenue\",\n",
    "        \"transaction_date\": \"source.transaction_date\",\n",
    "        \"customer_tier\": \"source.customer_tier\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"company_name\": \"source.company_name\",\n",
    "        \"revenue\": \"source.revenue\",\n",
    "        \"transaction_date\": \"source.transaction_date\",\n",
    "        \"customer_tier\": \"source.customer_tier\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"‚úÖ Merge complete!\")\n",
    "display(spark.read.format(\"delta\").load(delta_path).orderBy(\"customer_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab0e5ceb-5d6a-4136-b278-5757ffa7989a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 10: Delete Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750bdb62-c78f-429a-9f95-afd16a4cbda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üóëÔ∏è Deleting customers with revenue < 80000...\")\n",
    "\n",
    "delta_table.delete(\"revenue < 80000\")\n",
    "\n",
    "print(\"‚úÖ Delete complete!\")\n",
    "display(spark.read.format(\"delta\").load(delta_path))\n",
    "\n",
    "# Can still access deleted data via Time Travel!\n",
    "print(\"\\nüïê But we can still see deleted data in previous versions:\")\n",
    "display(spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf21d5e-7615-4949-b579-4c3bbb6246fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 11: OPTIMIZE - Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba0ef2c-f352-4fac-b373-067006ffd631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîß Running OPTIMIZE to compact small files...\")\n",
    "\n",
    "# This command compacts small files into larger ones\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "print(\"‚úÖ Optimization complete!\")\n",
    "\n",
    "# Check file statistics\n",
    "print(\"\\nFile details:\")\n",
    "display(spark.sql(f\"DESCRIBE DETAIL delta.`{delta_path}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43857bcc-4d2e-4d2b-8245-b910a173670f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 12: Z-ORDER (Advanced Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a2ae31-034a-4195-9d27-4cd5f8f72eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üéØ Running Z-ORDER on revenue column for faster filtering...\")\n",
    "\n",
    "# Z-ORDER co-locates related data for better query performance\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}` ZORDER BY (revenue)\")\n",
    "\n",
    "print(\"‚úÖ Z-ORDER complete! Queries filtering by revenue will be faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dec3ec46-e677-4678-bce6-6b74a5f76fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 13: VACUUM - Cleanup Old Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8357805d-9139-4938-9458-28b625505cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üßπ Current transaction history:\")\n",
    "display(delta_table.history().select(\"version\", \"timestamp\"))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è VACUUM will delete files not required by versions older than retention period\")\n",
    "print(\"Default retention: 7 days (168 hours)\")\n",
    "\n",
    "# For demo purposes, we'll use 0 hours (NOT recommended in production!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "print(\"\\nüóëÔ∏è Running VACUUM with 0 hour retention (demo only)...\")\n",
    "delta_table.vacuum(0)\n",
    "\n",
    "print(\"‚úÖ VACUUM complete!\")\n",
    "\n",
    "# Try to read old version (will fail after VACUUM)\n",
    "try:\n",
    "    spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path).display()\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Version 0 files deleted by VACUUM: {str(e)[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad432c18-923b-47e9-b79e-58a44d715dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "‚úÖ **ACID guarantees** make Delta production-ready  \n",
    "‚úÖ **Time Travel** enables auditing and rollback  \n",
    "‚úÖ **Schema enforcement** prevents data quality issues  \n",
    "‚úÖ **Merge/Upsert** simplifies CDC patterns  \n",
    "‚úÖ **OPTIMIZE & Z-ORDER** improve query performance  \n",
    "‚úÖ **VACUUM** manages storage costs  \n",
    "\n",
    "**Production best practices:**\n",
    "- Enable `optimizeWrite` and `autoCompact`\n",
    "- Set VACUUM retention to 7-30 days\n",
    "- Use Z-ORDER on commonly filtered columns\n",
    "- Monitor table history and file sizes\n",
    "- Use MERGE for CDC instead of delete+insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "170b7c10-5464-48bc-85e0-ccd83f427085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup (optional)\n",
    "# dbutils.fs.rm(delta_path, recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_lake_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
